{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kin recognition (two genes model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple model of the evolution of altruism via kin selection / kin recognition. It is built as a slightly more complex version of the previous kin recognition model, where we now make kin recognition costly and optional.\n",
    "\n",
    "### features of the model shared with previous simple kin selection model\n",
    "\n",
    "Each generation, every agent in a population plays a donation game with another agent, then all agents reproduce as a function of their relative fitness and die.\n",
    "\n",
    "The donation game is a very simple game where player 1 can 'Cooperate', i.e. pay a cost of $c$ to give a benefit $b$ to player 2, or 'Defect'. Player 2 is a passive recipient.\n",
    "\n",
    "Reproduction is asexual. Every agent who gets to reproduce contributes an even number of offspring to the next generation. This means that every agent has at least one sibling, who is a perfect clone of itself.\n",
    "\n",
    "The rate of sibling encounter is manipulated by a parameter r, which regulates how likely an agent is to be paired with one of its (clonal) sibling, when playing the donation game. I.e. an agent plays the donation game with its sibling with probability r, and with a randomly selected stranger with probability 1-r.\n",
    "\n",
    "### optional kin recognition \n",
    "\n",
    "Agents now have two genes. The first gene determines an agent's behavioral strategy, and can take the following values:\n",
    "\n",
    "-Cooperate: always cooperate\n",
    "\n",
    "-Defect: always defect\n",
    "\n",
    "-Nepotist: if you can't recognize kin, always defect. If you can recognize kin, then cooperate when paired with a sibling, and defect when paired with a stranger\n",
    "\n",
    "The second gene determines whether the agent has the ability to recognize siblings. It can take the value \"R\" for Recognizer, and \"I\", for \"Ignorant\". We assume that the kin recognition ability is costly because of the extra resources it takes to build a nervous system capable of recognizing kin. Therefore every agent playing \"R\" pays a small cost $d$ at the start of their life cycle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### dynamics\n",
    "\n",
    "By playing around with the model, one can see that despite its cost, kin recognition often evolves. It is evolutionarily successful because of its co-evolution with the Nepotist strategy. Nepotists are willing to pay the cost of kin recognition when it is compensated by the gains that come from not having to indiscriminately help everyone.\n",
    "\n",
    "An interesting exercise is to manipulate the value of $r$, the base rate of sibling encounter, while keeping other parameters fixed (with $b$ and $c$ large relative to $d$). Then one typically sees that all strategies are possible. When $r$ is small, indiscriminate defection is the only successful strategy, because sibling encounters are too rare for a kin recognition neural machinery to be a worthwhile investment. When $r$ approaches 1, indiscriminate cooperation prevails, because the ability to recognize kin does not provide much extra information. Kin recognition (along with Nepotism) evolves for intermediate values of $r$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### things to try\n",
    "\n",
    "You can try to modify the parameters below and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost and benefit of helping\n",
    "b = 30\n",
    "c = -10\n",
    "\n",
    "#cost of neurodevelopment for Kin recognition\n",
    "d = -2\n",
    "\n",
    "#the initial strategy\n",
    "initial_strategy = [\"D\", \"I\"]\n",
    "\n",
    "#population size\n",
    "N = 100\n",
    "\n",
    "#number of generations\n",
    "g = 500\n",
    "\n",
    "#mutation rate\n",
    "mu = .01\n",
    "\n",
    "#rate of sibling interaction\n",
    "r = 1/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#initial fitness\n",
    "initial_fitness = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific game parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#the strategy space\n",
    "strategy_space = [[\"C\", \"D\", \"N\"],[\"R\", \"I\"]]\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the \"agent\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this tells the program what an agent looks like,\n",
    "#e.g. it has fitness, is able to mutate, etc\n",
    "class agent:\n",
    "    def __init__(self, name, parent_name, strategy, fitness):\n",
    "        self.name = name\n",
    "        self.strategy = strategy\n",
    "        self.fitness = fitness\n",
    "        self.parent_name = parent_name\n",
    "    \n",
    "    def mutate(self):\n",
    "        index = 0 if np.random.uniform() < 1/2 else 1\n",
    "        self.strategy[index] = np.random.choice(strategy_space[index])\n",
    "       \n",
    "            \n",
    "    def add_to_fitness(self, increment):\n",
    "        self.fitness += increment\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some key functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    #pop.clear()\n",
    "    return([agent(i, 0, initial_strategy, initial_fitness) for i in range(N)])\n",
    "    #for i in range(N):\n",
    "        #pop.append(agent(i, 0, initial_strategy, initial_fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the function to compute the population proportion\n",
    "#for a given strategy\n",
    "\n",
    "\n",
    "def average_X(X):\n",
    "    counter=0;\n",
    "    strategy_index = 0;\n",
    "    strategy_index = max(idx for idx, val in enumerate(strategy_space) if X in val)\n",
    "    counter = len([i for i in pop if i.strategy[strategy_index] == X])\n",
    "    return counter/N\n",
    "    #for i in range(N):\n",
    "     #   if pop[i].strategy[strategy_index] == X:\n",
    "      #      counter+=1\n",
    "    #return counter/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the successor function\n",
    "#used to pairs an element in an array with its successor in that array,\n",
    "#where the successor is either the next element,\n",
    "#or the 0th element, if the focal element is the last element in the array\n",
    "\n",
    "def successor(current, total):\n",
    "    if current < total:\n",
    "        return current + 1\n",
    "    if current == total:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes agents play games\n",
    "#assortment is non-random: agents are more likely to play with their\n",
    "#sibling, where the parameter r controls the degree of non-randomness\n",
    "#of the assortment\n",
    "\n",
    "def non_random_games():\n",
    "    \n",
    "    #this subfunction finds the sibling of a focal agent,\n",
    "    #and makes the two agents play a game\n",
    "    def find_sibling(i):\n",
    "        player1 = arena[i]\n",
    "        #sibling_found = False\n",
    "        #while sibling_found == False:\n",
    "        for j, val in enumerate(arena):\n",
    "        #for j in range(len(arena)):\n",
    "            #player2 = arena[j]\n",
    "            player2 = val\n",
    "            if pop[player1].parent_name == pop[player2].parent_name and i != j:\n",
    "                DG(player1, player2)\n",
    "                break\n",
    "                #if i != j:\n",
    "                    #DG(player1, player2)\n",
    "                    #sibling_found = True\n",
    "                    #break\n",
    "                        \n",
    "    #this subfunction finds a random other player,\n",
    "    #and makes the two agents play a game                    \n",
    "    def find_stranger(i):\n",
    "        index_player1 = i\n",
    "        index_player2 = successor(i, len(arena)-1)\n",
    "        player1 = arena[index_player1]\n",
    "        player2 = arena[index_player2]\n",
    "        if (index_player1 != index_player2):\n",
    "            DG(player1, player2)\n",
    "    \n",
    "    #creates an array where we put the names of every agent\n",
    "    #in the population\n",
    "    arena = list(range(N))\n",
    "    #arena = []\n",
    "    #for i in range(N):\n",
    "     #   arena.append(i) \n",
    "    #randomly shuffle the names inside the array\n",
    "    np.random.shuffle(arena)\n",
    "    \n",
    "    #for every name in the array, make the corresponding agent play a game\n",
    "    #with probability r, he is paired with its sibling\n",
    "    #with probability 1-r, he is paired with a random player\n",
    "    for i in range(len(arena)):\n",
    "        random_number = np.random.uniform()\n",
    "        if random_number < r :\n",
    "            find_sibling(i)\n",
    "        else:\n",
    "            find_stranger(i)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the donation game\n",
    "\n",
    "def DG(player1, player2):\n",
    "    cue = False\n",
    "    player1_strat = pop[player1].strategy[0]\n",
    "    player1_cog = pop[player1].strategy[1]\n",
    "    if pop[player1].parent_name == pop[player2].parent_name:\n",
    "        cue = True\n",
    "        \n",
    "    if player1_strat == \"C\":\n",
    "        pop[player2].add_to_fitness(b)\n",
    "        pop[player1].add_to_fitness(c)\n",
    "        \n",
    "    if (player1_strat == \"N\") & (cue == True) & (player1_cog == \"R\"):\n",
    "        pop[player2].add_to_fitness(b)\n",
    "        pop[player1].add_to_fitness(c)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#the selection function and its building blocks\n",
    "\n",
    "#standardize fitness so that agents with lowest fitness have fitness 1\n",
    "def standardization():\n",
    "        \n",
    "        min_fitness = min(x.fitness for x in pop)\n",
    "        for i in range(N):\n",
    "            pop[i].fitness += 1\n",
    "            pop[i].fitness -= min_fitness\n",
    "        #min = 10**5\n",
    "        #for i in range(N):\n",
    "        #    if pop[i].fitness < min:\n",
    "         #       min=pop[i].fitness\n",
    "        #for i in range(N):\n",
    "         #   pop[i].fitness += 1\n",
    "          #  pop[i].fitness -= min\n",
    "\n",
    "#each agent gets to put its name in a hat w times, where w is its fitness\n",
    "def lottery_prep():\n",
    "    #hat = []\n",
    "    #for i in range(N):\n",
    "     #   for j in range(pop[i].fitness):\n",
    "      #      hat.append(pop[i].name)\n",
    "    return hat\n",
    "\n",
    "#create the next generation by drawing the name of the parents from the hat\n",
    "def reproduction(hat):\n",
    "    #for every even number from 0 to N-1, \n",
    "    #we select a parent by drawing a name from its hat\n",
    "    #this parent makes two offspring, who get the same\n",
    "    #'parent's name' tag identifying them as siblings\n",
    "    temporary_pop=[]\n",
    "    for i in range(N):\n",
    "        if i%2 == 0:\n",
    "            name_drawn = np.random.choice(hat)\n",
    "            parent_name = name_drawn\n",
    "            hereditary_package = [parent_name, [pop[name_drawn].strategy[0], pop[name_drawn].strategy[1]]]\n",
    "            temporary_pop.append(agent(i, hereditary_package[0], hereditary_package[1], initial_fitness))\n",
    "            temporary_pop.append(agent(i+1, hereditary_package[0], hereditary_package[1], initial_fitness))\n",
    "    pop.clear()\n",
    "    for i in range(N):\n",
    "        pop.append(temporary_pop[i])\n",
    "\n",
    "    #del temporary_pop\n",
    "\n",
    "\n",
    "#the selection function\n",
    "\n",
    "def selection():\n",
    "\n",
    "    standardization()\n",
    "    hat=lottery_prep()\n",
    "    reproduction(hat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurodevelopment():\n",
    "    for i in range(N):\n",
    "        if pop[i].strategy[1]==\"R\":\n",
    "            pop[i].add_to_fitness(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the mutation function\n",
    "def mutation():\n",
    "    for i in range(N):\n",
    "        if (np.random.uniform() < mu):\n",
    "            pop[i].mutate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model and plot the dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a035b87fa27a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mneurodevelopment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnon_random_games\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mselection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mAverage_coop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1a93e81d68b8>\u001b[0m in \u001b[0;36mselection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mstandardization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mhat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlottery_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mreproduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1a93e81d68b8>\u001b[0m in \u001b[0;36mlottery_prep\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#each agent gets to put its name in a hat w times, where w is its fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlottery_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0magent_occurence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent_occurence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m#hat = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#for i in range(N):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1a93e81d68b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#each agent gets to put its name in a hat w times, where w is its fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlottery_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0magent_occurence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent_occurence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m#hat = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#for i in range(N):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "#Run model\n",
    "\n",
    "g=1500\n",
    "#these arrays records average strategy proportions across a run\n",
    "Average_coop = []\n",
    "Average_defect = []\n",
    "Average_nep = []\n",
    "Average_recog = []\n",
    "Average_indif = []\n",
    "#reproduction / mutation functions\n",
    "\n",
    "\n",
    "#initialize()\n",
    "pop = initialize()\n",
    "for i in range(g):\n",
    "    neurodevelopment()\n",
    "    non_random_games()\n",
    "    selection()\n",
    "    mutation()\n",
    "    Average_coop.append(average_X(\"C\"))\n",
    "    Average_defect.append(average_X(\"D\"))\n",
    "    Average_nep.append(average_X(\"N\"))\n",
    "    Average_recog.append(average_X(\"R\"))\n",
    "    Average_indif.append(average_X(\"I\"))\n",
    "plt.figure( figsize=( 15, 8 ) )\n",
    "\n",
    "plt.title('Gene frequencies over time')\n",
    "\n",
    "plt.ylabel('Gene frequencies')\n",
    "plt.xlabel('Generation')\n",
    "plt.plot(Average_coop)\n",
    "plt.plot(Average_defect, color=\"orange\")\n",
    "plt.plot(Average_nep, color=\"green\")\n",
    "plt.plot(Average_recog, color=\"purple\")\n",
    "plt.ylim(0,1.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph legend\n",
    "\n",
    "Behavioral strategies: \n",
    "\n",
    "Blue = Cooperator; Orange = Defector; Green = Nepotist\n",
    "\n",
    "The Purple curve is the proportion of Recognizers in the population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3&4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
